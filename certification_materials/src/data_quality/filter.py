#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®è´¨é‡ç­›é€‰å™¨

æä¾›é…’åº—æ•°æ®è´¨é‡ç­›é€‰çš„æ ¸å¿ƒåŠŸèƒ½
"""

import json
import time
import re
from typing import List, Dict, Any, Tuple, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock

from ..utils import VertexAIManager, setup_logger
from .rules import DataQualityRules, ProcessingConfig, FilterStats, FilterResult


class AIQualityEvaluator:
    """AIè´¨é‡è¯„ä¼°å™¨"""
    
    def __init__(self, ai_manager: VertexAIManager):
        """
        åˆå§‹åŒ–AIè´¨é‡è¯„ä¼°å™¨
        
        Args:
            ai_manager: VertexAIç®¡ç†å™¨å®ä¾‹
        """
        self.ai_manager = ai_manager
        self.logger = setup_logger("ai_evaluator")
    
    def create_evaluation_prompt(self, entries_data: List[Tuple[str, str]]) -> str:
        """
        åˆ›å»ºè¯„ä¼°æç¤ºè¯
        
        Args:
            entries_data: é—®ç­”å¯¹åˆ—è¡¨
            
        Returns:
            è¯„ä¼°æç¤ºè¯
        """
        batch_items = []
        for idx, (question, answer) in enumerate(entries_data):
            # æˆªæ–­è¿‡é•¿å†…å®¹ä»¥æé«˜å¤„ç†é€Ÿåº¦
            truncated_question = question[:100] if len(question) > 100 else question
            truncated_answer = answer[:300] if len(answer) > 300 else answer
            
            batch_items.append(f"[{idx+1}] Q: {truncated_question}\nA: {truncated_answer}")
        
        prompt = f"""è¯·è¯„ä¼°ä»¥ä¸‹{len(batch_items)}ä¸ªé…’åº—æœåŠ¡é—®ç­”å¯¹çš„è´¨é‡ã€‚
è¯„åˆ†æ ‡å‡†ï¼š1-10åˆ†ï¼Œ7åˆ†åŠä»¥ä¸Šä¸ºé«˜è´¨é‡æ•°æ®ã€‚

è¯„ä¼°è¦ç‚¹ï¼š
1. é—®é¢˜æ˜¯å¦æ¸…æ™°å…·ä½“ä¸”ä¸é…’åº—æœåŠ¡ç›¸å…³ï¼Ÿ
2. å›ç­”æ˜¯å¦ä¸“ä¸šå‡†ç¡®ä¸”æœ‰å®ç”¨ä»·å€¼ï¼Ÿ
3. é—®ç­”æ˜¯å¦åŒ¹é…ä¸”é€»è¾‘åˆç†ï¼Ÿ

{chr(10).join(batch_items)}

è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ•°ç»„æ ¼å¼å›å¤ï¼š
[{{"score":8,"keep":true}},{{"score":6,"keep":false}}...]"""
        
        return prompt
    
    def parse_ai_response(self, response_text: str, entry_count: int) -> List[Dict[str, Any]]:
        """
        è§£æAIå“åº”
        
        Args:
            response_text: AIå“åº”æ–‡æœ¬
            entry_count: æ¡ç›®æ•°é‡
            
        Returns:
            è§£æåçš„è¯„ä¼°ç»“æœåˆ—è¡¨
        """
        try:
            # æå–JSONæ•°ç»„
            json_match = re.search(r'\[.*?\]', response_text, re.DOTALL)
            if json_match:
                evaluations = json.loads(json_match.group(0))
            else:
                # å¤‡ç”¨æ–¹æ¡ˆï¼šåˆ›å»ºé»˜è®¤è¯„ä¼°
                evaluations = [{'score': 7, 'keep': True} for _ in range(entry_count)]
                self.logger.warning(f"æ— æ³•è§£æAIå“åº”ï¼Œä½¿ç”¨é»˜è®¤è¯„ä¼°")
            
            # æ ‡å‡†åŒ–è¯„ä¼°ç»“æœ
            standardized_results = []
            for i in range(entry_count):
                if i < len(evaluations):
                    eval_result = evaluations[i]
                    score = eval_result.get('score', 7)
                    keep = eval_result.get('keep', score >= 7)
                else:
                    score = 7
                    keep = True
                
                standardized_results.append({
                    'score': score,
                    'keep': keep,
                    'reason': f'AIè¯„ä¼°(åˆ†æ•°:{score})'
                })
            
            return standardized_results
            
        except Exception as e:
            # è§£æå¤±è´¥æ—¶çš„å¤‡ç”¨æ–¹æ¡ˆ
            self.logger.error(f"è§£æAIå“åº”å¤±è´¥: {e}")
            return [{'score': 7, 'keep': True, 'reason': f'è§£æå¤±è´¥: {str(e)}'} 
                   for _ in range(entry_count)]
    
    def evaluate_batch(self, entries: List[Tuple[int, Dict[str, Any]]], 
                      batch_id: int) -> List[Tuple[int, Dict[str, Any]]]:
        """
        æ‰¹é‡è¯„ä¼°æ•°æ®è´¨é‡
        
        Args:
            entries: æ•°æ®æ¡ç›®åˆ—è¡¨ [(åŸå§‹ç´¢å¼•, æ•°æ®)]
            batch_id: æ‰¹æ¬¡ID
            
        Returns:
            è¯„ä¼°ç»“æœåˆ—è¡¨ [(åŸå§‹ç´¢å¼•, è¯„ä¼°ç»“æœ)]
        """
        if not entries:
            return []
        
        # æå–é—®ç­”æ•°æ®
        entries_data = []
        valid_entries = []
        
        for original_index, entry in entries:
            try:
                contents = entry.get('contents', [])
                if len(contents) < 2:
                    continue
                
                question = contents[0]['parts'][0]['text']
                answer = contents[1]['parts'][0]['text']
                
                entries_data.append((question, answer))
                valid_entries.append((original_index, entry))
                
            except Exception as e:
                self.logger.warning(f"è·³è¿‡æ— æ•ˆæ¡ç›® {original_index}: {e}")
                continue
        
        if not entries_data:
            return [(idx, {'score': 5, 'keep': True, 'reason': 'æ•°æ®æ ¼å¼é”™è¯¯'}) 
                   for idx, _ in entries]
        
        try:
            # ç”Ÿæˆè¯„ä¼°æç¤ºè¯å¹¶è·å–AIå“åº”
            prompt = self.create_evaluation_prompt(entries_data)
            response_text = self.ai_manager.generate_content(prompt)
            
            # è§£æå“åº”
            evaluations = self.parse_ai_response(response_text, len(valid_entries))
            
            # æ„å»ºç»“æœ
            results = []
            for i, (original_index, entry) in enumerate(valid_entries):
                evaluation = evaluations[i]
                evaluation['reason'] = f"AIè¯„ä¼°-æ‰¹æ¬¡{batch_id}(åˆ†æ•°:{evaluation['score']})"
                results.append((original_index, evaluation))
            
            return results
            
        except Exception as e:
            # è¯„ä¼°å¤±è´¥æ—¶çš„ä¿å®ˆç­–ç•¥
            self.logger.error(f"æ‰¹æ¬¡{batch_id}è¯„ä¼°å¤±è´¥: {e}")
            return [(idx, {'score': 6, 'keep': True, 'reason': f'è¯„ä¼°å¤±è´¥-æ‰¹æ¬¡{batch_id}: {str(e)}'}) 
                   for idx, _ in valid_entries]


class HotelDataQualityFilter:
    """é…’åº—æ•°æ®è´¨é‡ç­›é€‰å™¨ä¸»ç±»"""
    
    def __init__(self, credentials_path: Optional[str] = None, 
                 project_id: Optional[str] = None,
                 model_name: str = "gemini-2.5-flash",
                 log_level: str = "INFO"):
        """
        åˆå§‹åŒ–ç­›é€‰å™¨
        
        Args:
            credentials_path: æœåŠ¡è´¦æˆ·å¯†é’¥æ–‡ä»¶è·¯å¾„
            project_id: Google Cloudé¡¹ç›®ID
            model_name: ä½¿ç”¨çš„æ¨¡å‹åç§°
            log_level: æ—¥å¿—çº§åˆ«
        """
        self.logger = setup_logger("hotel_data_filter", level=log_level)
        self.logger.info("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–é…’åº—æ•°æ®è´¨é‡ç­›é€‰å™¨...")
        
        # åˆå§‹åŒ–AIç®¡ç†å™¨
        self.ai_manager = VertexAIManager(
            credentials_path=credentials_path,
            project_id=project_id,
            model_name=model_name
        )
        
        # åˆå§‹åŒ–AIè¯„ä¼°å™¨
        self.ai_evaluator = AIQualityEvaluator(self.ai_manager)
        
        # çº¿ç¨‹å®‰å…¨
        self.processing_lock = Lock()
        self.processed_count = 0
        self.start_time = time.time()
    
    def load_data(self, input_file: str, sample_size: Optional[int] = None) -> List[Dict[str, Any]]:
        """
        åŠ è½½æ•°æ®
        
        Args:
            input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
            sample_size: æ ·æœ¬å¤§å°é™åˆ¶
            
        Returns:
            æ•°æ®æ¡ç›®åˆ—è¡¨
        """
        self.logger.info(f"ğŸ“– æ­£åœ¨åŠ è½½æ•°æ®: {input_file}")
        
        data_entries = []
        with open(input_file, 'r', encoding='utf-8') as file:
            for line_number, line in enumerate(file, 1):
                try:
                    entry = json.loads(line.strip())
                    data_entries.append(entry)
                    
                    # å¦‚æœæŒ‡å®šäº†æ ·æœ¬å¤§å°ï¼Œåˆ™é™åˆ¶åŠ è½½æ•°é‡
                    if sample_size and len(data_entries) >= sample_size:
                        self.logger.info(f"ğŸ” ä½¿ç”¨æ ·æœ¬æ•°æ®: {sample_size} æ¡ç›®")
                        break
                        
                except json.JSONDecodeError:
                    self.logger.warning(f"âš ï¸ è·³è¿‡æ— æ•ˆJSONè¡Œ: {line_number}")
                    continue
        
        self.logger.info(f"ğŸ“Š æˆåŠŸåŠ è½½ {len(data_entries)} æ¡æ•°æ®")
        return data_entries
    
    def apply_rule_filtering(self, data_entries: List[Dict[str, Any]]) -> Tuple[List[Tuple[int, Dict[str, Any]]], List[FilterResult]]:
        """
        åº”ç”¨è§„åˆ™ç­›é€‰
        
        Args:
            data_entries: æ•°æ®æ¡ç›®åˆ—è¡¨
            
        Returns:
            (é€šè¿‡è§„åˆ™ç­›é€‰çš„æ¡ç›®, æ‰€æœ‰ç­›é€‰ç»“æœ)
        """
        self.logger.info("âš¡ é˜¶æ®µ1: è§„åˆ™é¢„ç­›é€‰...")
        rule_start_time = time.time()
        
        passed_entries = []
        all_results = []
        
        for index, entry in enumerate(data_entries):
            is_valid, reason, stats = DataQualityRules.validate_entry(entry)
            
            result = FilterResult(
                index=index,
                is_kept=is_valid,
                score=0 if not is_valid else 5,  # è§„åˆ™ç­›é€‰é˜¶æ®µæš‚æ—¶ç»™5åˆ†
                reason=reason,
                stage='rule_filter',
                stats=stats
            )
            all_results.append(result)
            
            if is_valid:
                passed_entries.append((index, entry))
        
        rule_duration = time.time() - rule_start_time
        failed_count = len(data_entries) - len(passed_entries)
        pass_rate = len(passed_entries) / len(data_entries) * 100
        
        self.logger.info(f"âœ… è§„åˆ™ç­›é€‰å®Œæˆ ({rule_duration:.1f}ç§’)")
        self.logger.info(f"ğŸ“Š é€šè¿‡: {len(passed_entries)} | æ·˜æ±°: {failed_count} | é€šè¿‡ç‡: {pass_rate:.1f}%")
        
        return passed_entries, all_results
    
    def apply_ai_evaluation(self, passed_entries: List[Tuple[int, Dict[str, Any]]], 
                           config: ProcessingConfig) -> Dict[int, Dict[str, Any]]:
        """
        åº”ç”¨AIè¯„ä¼°
        
        Args:
            passed_entries: é€šè¿‡è§„åˆ™ç­›é€‰çš„æ¡ç›®
            config: å¤„ç†é…ç½®
            
        Returns:
            AIè¯„ä¼°ç»“æœå­—å…¸
        """
        if not passed_entries:
            return {}
        
        self.logger.info(f"ğŸ¤– é˜¶æ®µ2: AIæ‰¹é‡è¯„ä¼° {len(passed_entries)} æ¡ç›®...")
        ai_start_time = time.time()
        
        # åˆ†æ‰¹å¤„ç†
        batches = [passed_entries[i:i + config.batch_size] 
                  for i in range(0, len(passed_entries), config.batch_size)]
        self.logger.info(f"ğŸ“¦ åˆ†æˆ {len(batches)} ä¸ªæ‰¹æ¬¡å¤„ç†")
        
        all_evaluations = {}
        
        with ThreadPoolExecutor(max_workers=config.max_workers) as executor:
            # æäº¤æ‰€æœ‰æ‰¹æ¬¡ä»»åŠ¡
            future_to_batch = {
                executor.submit(self.ai_evaluator.evaluate_batch, batch, batch_id): (batch_id, batch)
                for batch_id, batch in enumerate(batches)
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_batch):
                try:
                    batch_results = future.result()
                    for original_index, evaluation in batch_results:
                        all_evaluations[original_index] = evaluation
                    
                    # æ›´æ–°è¿›åº¦
                    with self.processing_lock:
                        self.processed_count += len(batch_results)
                        elapsed_time = time.time() - self.start_time
                        processing_rate = self.processed_count / elapsed_time if elapsed_time > 0 else 0
                        self.logger.info(f"ğŸ“Š å·²å¤„ç†: {self.processed_count} æ¡ç›® | é€Ÿåº¦: {processing_rate:.1f} æ¡ç›®/ç§’")
                
                except Exception as e:
                    batch_id, batch = future_to_batch[future]
                    self.logger.error(f"âŒ æ‰¹æ¬¡{batch_id}å¤„ç†å¤±è´¥: {e}")
                    # å¤±è´¥æ—¶é‡‡ç”¨ä¿å®ˆç­–ç•¥
                    for original_index, _ in batch:
                        all_evaluations[original_index] = {
                            'score': 6, 'keep': True, 'reason': f'æ‰¹æ¬¡{batch_id}å¤„ç†å¤±è´¥'
                        }
        
        ai_duration = time.time() - ai_start_time
        self.logger.info(f"âœ… AIè¯„ä¼°å®Œæˆ ({ai_duration:.1f}ç§’)")
        
        return all_evaluations
    
    def merge_results(self, data_entries: List[Dict[str, Any]], 
                     rule_results: List[FilterResult],
                     ai_evaluations: Dict[int, Dict[str, Any]],
                     config: ProcessingConfig) -> Tuple[List[FilterResult], List[FilterResult]]:
        """
        åˆå¹¶ç­›é€‰ç»“æœ
        
        Args:
            data_entries: åŸå§‹æ•°æ®æ¡ç›®
            rule_results: è§„åˆ™ç­›é€‰ç»“æœ
            ai_evaluations: AIè¯„ä¼°ç»“æœ
            config: å¤„ç†é…ç½®
            
        Returns:
            (é«˜è´¨é‡ç»“æœ, ä½è´¨é‡ç»“æœ)
        """
        self.logger.info("ğŸ“‹ æ­£åœ¨åˆå¹¶ç­›é€‰ç»“æœ...")
        
        high_quality_results = []
        low_quality_results = []
        
        for index, entry in enumerate(data_entries):
            rule_result = rule_results[index]
            
            if not rule_result.is_kept:
                # è§„åˆ™ç­›é€‰å¤±è´¥
                low_quality_results.append(rule_result)
            else:
                # è·å–AIè¯„ä¼°ç»“æœ
                ai_evaluation = ai_evaluations.get(index, {
                    'score': 5, 'keep': True, 'reason': 'æœªè¿›è¡ŒAIè¯„ä¼°'
                })
                
                final_result = FilterResult(
                    index=index,
                    is_kept=ai_evaluation.get('keep', True) and ai_evaluation.get('score', 5) >= config.min_score,
                    score=ai_evaluation.get('score', 5),
                    reason=ai_evaluation.get('reason', ''),
                    stage='ai_evaluation',
                    stats=rule_result.stats
                )
                
                if final_result.is_kept:
                    high_quality_results.append(final_result)
                else:
                    low_quality_results.append(final_result)
        
        return high_quality_results, low_quality_results
    
    def save_results(self, data_entries: List[Dict[str, Any]], 
                    high_quality_results: List[FilterResult],
                    output_file: str):
        """
        ä¿å­˜ç­›é€‰ç»“æœ
        
        Args:
            data_entries: åŸå§‹æ•°æ®æ¡ç›®
            high_quality_results: é«˜è´¨é‡ç»“æœ
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        self.logger.info("ğŸ’¾ æ­£åœ¨ä¿å­˜ç­›é€‰ç»“æœ...")
        
        with open(output_file, 'w', encoding='utf-8') as file:
            for result in high_quality_results:
                entry_data = data_entries[result.index]
                json.dump(entry_data, file, ensure_ascii=False)
                file.write('\n')
        
        self.logger.info(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    def generate_report(self, data_entries: List[Dict[str, Any]],
                       high_quality_results: List[FilterResult],
                       low_quality_results: List[FilterResult],
                       config: ProcessingConfig,
                       output_file: str) -> Dict[str, Any]:
        """
        ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        
        Args:
            data_entries: åŸå§‹æ•°æ®æ¡ç›®
            high_quality_results: é«˜è´¨é‡ç»“æœ
            low_quality_results: ä½è´¨é‡ç»“æœ
            config: å¤„ç†é…ç½®
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            
        Returns:
            æŠ¥å‘Šæ•°æ®
        """
        total_processing_time = time.time() - self.start_time
        total_count = len(data_entries)
        high_quality_count = len(high_quality_results)
        retention_rate = high_quality_count / total_count * 100
        processing_speed = total_count / total_processing_time
        
        # ç»Ÿè®¡å„é˜¶æ®µç»“æœ
        rule_failed_count = len([r for r in low_quality_results if r.stage == 'rule_filter'])
        ai_failed_count = len([r for r in low_quality_results if r.stage == 'ai_evaluation'])
        
        # è·å–è´¨é‡ç»Ÿè®¡æ‘˜è¦
        all_results = high_quality_results + low_quality_results
        quality_summary = DataQualityRules.get_quality_summary(all_results)
        
        report = {
            'summary': {
                'total_entries': total_count,
                'high_quality_entries': high_quality_count,
                'low_quality_entries': len(low_quality_results),
                'retention_rate_percent': retention_rate,
                'processing_time_seconds': total_processing_time,
                'processing_speed_per_second': processing_speed
            },
            'configuration': {
                'minimum_score': config.min_score,
                'batch_size': config.batch_size,
                'max_workers': config.max_workers,
                'sample_size': config.sample_size,
                'model_info': self.ai_manager.get_model_info()
            },
            'stage_statistics': {
                'rule_filter_failed': rule_failed_count,
                'ai_evaluation_failed': ai_failed_count,
                'rule_filter_passed': total_count - rule_failed_count,
                'ai_evaluation_passed': high_quality_count
            },
            'quality_analysis': quality_summary
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = output_file.replace('.jsonl', '_quality_report.json')
        with open(report_file, 'w', encoding='utf-8') as file:
            json.dump(report, file, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ğŸ“‹ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        return report
    
    def filter_data(self, input_file: str, output_file: str, config: ProcessingConfig):
        """
        æ‰§è¡Œå®Œæ•´çš„æ•°æ®ç­›é€‰æµç¨‹
        
        Args:
            input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            config: å¤„ç†é…ç½®
        """
        self.logger.info(f"ğŸš€ å¼€å§‹é…’åº—æ•°æ®è´¨é‡ç­›é€‰")
        self.logger.info(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {input_file}")
        self.logger.info(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}")
        self.logger.info(f"âš™ï¸ é…ç½®: æœ€ä½åˆ†æ•°={config.min_score}, æ‰¹é‡å¤§å°={config.batch_size}, å¹¶å‘æ•°={config.max_workers}")
        
        try:
            # 1. åŠ è½½æ•°æ®
            data_entries = self.load_data(input_file, config.sample_size)
            
            # 2. è§„åˆ™ç­›é€‰
            passed_entries, rule_results = self.apply_rule_filtering(data_entries)
            
            if not passed_entries:
                self.logger.error("âŒ æ²¡æœ‰æ•°æ®é€šè¿‡è§„åˆ™ç­›é€‰ï¼Œç¨‹åºç»ˆæ­¢")
                return
            
            # 3. AIè¯„ä¼°
            ai_evaluations = self.apply_ai_evaluation(passed_entries, config)
            
            # 4. åˆå¹¶ç»“æœ
            high_quality_results, low_quality_results = self.merge_results(
                data_entries, rule_results, ai_evaluations, config
            )
            
            # 5. ä¿å­˜ç»“æœ
            self.save_results(data_entries, high_quality_results, output_file)
            
            # 6. ç”ŸæˆæŠ¥å‘Š
            report = self.generate_report(
                data_entries, high_quality_results, low_quality_results, config, output_file
            )
            
            # 7. è¾“å‡ºæ€»ç»“
            self.logger.info(f"\nğŸ‰ æ•°æ®ç­›é€‰å®Œæˆï¼")
            self.logger.info(f"â±ï¸  æ€»è€—æ—¶: {report['summary']['processing_time_seconds']:.1f}ç§’")
            self.logger.info(f"ğŸš€ å¤„ç†é€Ÿåº¦: {report['summary']['processing_speed_per_second']:.1f} æ¡ç›®/ç§’")
            self.logger.info(f"ğŸ“Š åŸå§‹æ•°æ®: {report['summary']['total_entries']} æ¡ç›®")
            self.logger.info(f"ğŸ“Š é«˜è´¨é‡æ•°æ®: {report['summary']['high_quality_entries']} æ¡ç›® ({report['summary']['retention_rate_percent']:.1f}%)")
            self.logger.info(f"ğŸ“Š ä½è´¨é‡æ•°æ®: {report['summary']['low_quality_entries']} æ¡ç›®")
            self.logger.info(f"ğŸ“ ç­›é€‰ç»“æœ: {output_file}")
            
        except Exception as e:
            self.logger.error(f"âŒ æ•°æ®ç­›é€‰è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            raise 