#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Gemini 1.5 Pro é…’åº—æœåŠ¡å¾®è°ƒè„šæœ¬ - Vertex AI Workbench æœ€ç»ˆç‰ˆæœ¬
ä½¿ç”¨ä¼˜åŒ–åçš„JSONLæ•°æ®é›†è¿›è¡Œç›‘ç£å¾®è°ƒ
"""

import os
import sys
import time
import json
import subprocess
from typing import Optional, Tuple
from pathlib import Path

# å®‰è£…å¿…è¦çš„åŒ…
def install_requirements():
    """å®‰è£…å¿…è¦çš„PythonåŒ…"""
    try:
        subprocess.run([
            sys.executable, "-m", "pip", "install", 
            "--upgrade", "--quiet", "google-cloud-aiplatform", "google-cloud-storage"
        ], check=True)
        print("âœ… å¿…è¦åŒ…å®‰è£…å®Œæˆ")
    except subprocess.CalledProcessError as e:
        print(f"âŒ åŒ…å®‰è£…å¤±è´¥: {e}")
        sys.exit(1)

# å¯¼å…¥å¿…è¦çš„åº“
try:
    import vertexai
    from vertexai.generative_models import GenerativeModel
    from vertexai.preview.tuning import sft
    from google.cloud import storage
    from google.auth import default
except ImportError:
    print("æ­£åœ¨å®‰è£…å¿…è¦çš„åŒ…...")
    install_requirements()
    import vertexai
    from vertexai.generative_models import GenerativeModel
    from vertexai.preview.tuning import sft
    from google.cloud import storage
    from google.auth import default

# é¡¹ç›®é…ç½® - åœ¨Workbenchä¸­è‡ªåŠ¨è·å–
def get_project_config():
    """è·å–é¡¹ç›®é…ç½®ä¿¡æ¯"""
    try:
        # åœ¨Workbenchä¸­è‡ªåŠ¨è·å–é¡¹ç›®ä¿¡æ¯
        credentials, project_id = default()
        if not project_id:
            project_id = "cy-aispeci-demo"  # å¤‡ç”¨é¡¹ç›®ID
        
        location = "asia-east2"
        bucket_name = "peft-model-cy-aispeci-demo"
        
        print(f"ğŸ“‹ é¡¹ç›®é…ç½®:")
        print(f"   é¡¹ç›®ID: {project_id}")
        print(f"   åŒºåŸŸ: {location}")
        print(f"   å­˜å‚¨æ¡¶: gs://{bucket_name}")
        
        return project_id, location, bucket_name
        
    except Exception as e:
        print(f"âŒ è·å–é¡¹ç›®é…ç½®å¤±è´¥: {e}")
        # ä½¿ç”¨é»˜è®¤é…ç½®
        return "cy-aispeci-demo", "asia-east2", "peft-model-cy-aispeci-demo"

def validate_jsonl_file(file_path: str) -> Tuple[bool, int]:
    """éªŒè¯JSONLæ–‡ä»¶æ ¼å¼"""
    print(f"ğŸ” éªŒè¯æ–‡ä»¶: {file_path}")
    
    if not os.path.exists(file_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return False, 0
    
    try:
        count = 0
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                if line.strip():
                    try:
                        data = json.loads(line.strip())
                        
                        # éªŒè¯Geminiæ ¼å¼
                        if not all(key in data for key in ['systemInstruction', 'contents']):
                            print(f"âŒ ç¬¬ {line_num} è¡Œç¼ºå°‘å¿…è¦å­—æ®µ")
                            return False, 0
                        
                        # éªŒè¯ç³»ç»ŸæŒ‡ä»¤æ ¼å¼
                        sys_inst = data['systemInstruction']
                        if sys_inst.get('role') != 'system' or 'parts' not in sys_inst:
                            print(f"âŒ ç¬¬ {line_num} è¡Œç³»ç»ŸæŒ‡ä»¤æ ¼å¼é”™è¯¯")
                            return False, 0
                        
                        # éªŒè¯å¯¹è¯å†…å®¹æ ¼å¼
                        contents = data['contents']
                        if len(contents) < 2:
                            print(f"âŒ ç¬¬ {line_num} è¡Œå¯¹è¯å†…å®¹ä¸å®Œæ•´")
                            return False, 0
                        
                        if contents[0].get('role') != 'user' or contents[1].get('role') != 'model':
                            print(f"âŒ ç¬¬ {line_num} è¡Œå¯¹è¯è§’è‰²é”™è¯¯")
                            return False, 0
                        
                        count += 1
                        
                    except json.JSONDecodeError as e:
                        print(f"âŒ ç¬¬ {line_num} è¡ŒJSONæ ¼å¼é”™è¯¯: {e}")
                        return False, 0
        
        print(f"âœ… æ–‡ä»¶éªŒè¯é€šè¿‡ï¼Œå…± {count} æ¡è®°å½•")
        return True, count
        
    except Exception as e:
        print(f"âŒ æ–‡ä»¶éªŒè¯å¤±è´¥: {e}")
        return False, 0

def upload_to_gcs(local_file: str, bucket_name: str, gcs_path: str) -> bool:
    """ä¸Šä¼ æ–‡ä»¶åˆ°Google Cloud Storage"""
    try:
        print(f"ğŸ“¤ ä¸Šä¼ æ–‡ä»¶åˆ°GCS: {local_file} -> gs://{bucket_name}/{gcs_path}")
        
        storage_client = storage.Client()
        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(gcs_path)
        
        # ä¸Šä¼ æ–‡ä»¶
        blob.upload_from_filename(local_file)
        
        print(f"âœ… æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: gs://{bucket_name}/{gcs_path}")
        return True
        
    except Exception as e:
        print(f"âŒ æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {e}")
        return False

def check_bucket_exists(bucket_name: str) -> bool:
    """æ£€æŸ¥å­˜å‚¨æ¡¶æ˜¯å¦å­˜åœ¨"""
    try:
        storage_client = storage.Client()
        bucket = storage_client.bucket(bucket_name)
        bucket.reload()
        print(f"âœ… å­˜å‚¨æ¡¶å­˜åœ¨: gs://{bucket_name}")
        return True
    except Exception as e:
        print(f"âŒ å­˜å‚¨æ¡¶ä¸å­˜åœ¨æˆ–æ— æ³•è®¿é—®: {e}")
        return False

def start_tuning_job(project_id: str, location: str, train_uri: str, validation_uri: str) -> Optional[object]:
    """å¯åŠ¨å¾®è°ƒä»»åŠ¡"""
    try:
        print("ğŸš€ å¯åŠ¨Geminiæ¨¡å‹å¾®è°ƒä»»åŠ¡...")
        
        # åˆå§‹åŒ–Vertex AI
        vertexai.init(project=project_id, location=location)
        
        # åˆ›å»ºå¾®è°ƒä»»åŠ¡
        sft_tuning_job = sft.train(
            source_model="gemini-1.5-pro-002",
            train_dataset=train_uri,
            validation_dataset=validation_uri,
            epochs=3,
            learning_rate_multiplier=1.0,
            tuned_model_display_name="é…’åº—æœåŠ¡åŠ©æ‰‹ - Gemini 1.5 Pro å¾®è°ƒ"
        )
        
        print("âœ… å¾®è°ƒä»»åŠ¡å·²å¯åŠ¨")
        print(f"ğŸ“‹ ä»»åŠ¡ä¿¡æ¯:")
        print(f"   ä»»åŠ¡åç§°: {sft_tuning_job.resource_name}")
        print(f"   åŸºç¡€æ¨¡å‹: gemini-1.5-pro-002")
        print(f"   è®­ç»ƒè½®æ•°: 3")
        print(f"   å­¦ä¹ ç‡å€æ•°: 1.0")
        
        return sft_tuning_job
        
    except Exception as e:
        print(f"âŒ å¯åŠ¨å¾®è°ƒä»»åŠ¡å¤±è´¥: {e}")
        return None

def monitor_tuning_job(sft_tuning_job) -> bool:
    """ç›‘æ§å¾®è°ƒä»»åŠ¡è¿›åº¦"""
    try:
        print("â³ ç›‘æ§å¾®è°ƒä»»åŠ¡è¿›åº¦...")
        print("ğŸ“ å¾®è°ƒé€šå¸¸éœ€è¦ 60-120 åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...")
        
        start_time = time.time()
        
        while not sft_tuning_job.refresh().has_ended:
            elapsed_time = time.time() - start_time
            elapsed_minutes = int(elapsed_time // 60)
            
            print(f"â±ï¸  å·²è¿è¡Œ {elapsed_minutes} åˆ†é’Ÿï¼Œä»»åŠ¡ä»åœ¨è¿›è¡Œä¸­...")
            
            # æ¯5åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
            time.sleep(300)
        
        # æ£€æŸ¥ä»»åŠ¡çŠ¶æ€
        job_state = sft_tuning_job.state
        print(f"ğŸ“Š å¾®è°ƒä»»åŠ¡å®Œæˆï¼ŒçŠ¶æ€: {job_state}")
        
        if job_state.name == 'JOB_STATE_SUCCEEDED':
            print("âœ… å¾®è°ƒä»»åŠ¡æˆåŠŸå®Œæˆï¼")
            
            # è·å–å¾®è°ƒåçš„æ¨¡å‹ä¿¡æ¯
            tuned_model_name = sft_tuning_job.tuned_model_name
            tuned_model_endpoint = sft_tuning_job.tuned_model_endpoint_name
            
            print(f"ğŸ“‹ å¾®è°ƒåçš„æ¨¡å‹ä¿¡æ¯:")
            print(f"   æ¨¡å‹åç§°: {tuned_model_name}")
            print(f"   ç«¯ç‚¹åç§°: {tuned_model_endpoint}")
            
            return True
        else:
            print(f"âŒ å¾®è°ƒä»»åŠ¡å¤±è´¥ï¼ŒçŠ¶æ€: {job_state}")
            return False
            
    except Exception as e:
        print(f"âŒ ç›‘æ§å¾®è°ƒä»»åŠ¡å¤±è´¥: {e}")
        return False

def test_tuned_model(sft_tuning_job) -> bool:
    """æµ‹è¯•å¾®è°ƒåçš„æ¨¡å‹"""
    try:
        print("ğŸ§ª æµ‹è¯•å¾®è°ƒåçš„æ¨¡å‹...")
        
        # è·å–å¾®è°ƒåçš„æ¨¡å‹ç«¯ç‚¹
        tuned_model_endpoint = sft_tuning_job.tuned_model_endpoint_name
        tuned_model = GenerativeModel(tuned_model_endpoint)
        
        # æµ‹è¯•é—®é¢˜
        test_questions = [
            "å¦‚ä½•æé«˜é…’åº—å®¢æˆ¿æ¸…æ´æ•ˆç‡ï¼Ÿ",
            "é…’åº—å‰å°å¦‚ä½•å¤„ç†å®¢æˆ·æŠ•è¯‰ï¼Ÿ",
            "å¦‚ä½•ä¼˜åŒ–é…’åº—çš„å®¢æˆ·ä½“éªŒï¼Ÿ"
        ]
        
        print("ğŸ“ æµ‹è¯•ç»“æœ:")
        
        for i, question in enumerate(test_questions, 1):
            print(f"\n--- æµ‹è¯• {i} ---")
            print(f"é—®é¢˜: {question}")
            
            try:
                response = tuned_model.generate_content(question)
                answer = response.text[:300] + "..." if len(response.text) > 300 else response.text
                print(f"å›ç­”: {answer}")
                
            except Exception as e:
                print(f"âŒ ç”Ÿæˆå›ç­”å¤±è´¥: {e}")
        
        print("\nâœ… æ¨¡å‹æµ‹è¯•å®Œæˆ")
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¨ Gemini 1.5 Pro é…’åº—æœåŠ¡å¾®è°ƒ - Workbenchç‰ˆæœ¬")
    print("=" * 60)
    
    # è·å–é¡¹ç›®é…ç½®
    project_id, location, bucket_name = get_project_config()
    
    # æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆä½¿ç”¨ä¼˜åŒ–åçš„æ•°æ®é›†ï¼‰
    train_file = "./dataset/optimized_train.jsonl"
    validation_file = "./dataset/optimized_validation.jsonl"
    
    print(f"\nğŸ“‚ æ•°æ®æ–‡ä»¶:")
    print(f"   è®­ç»ƒé›†: {train_file}")
    print(f"   éªŒè¯é›†: {validation_file}")
    
    # éªŒè¯æ•°æ®æ–‡ä»¶
    print("\nğŸ” éªŒè¯æ•°æ®æ–‡ä»¶...")
    train_valid, train_count = validate_jsonl_file(train_file)
    validation_valid, validation_count = validate_jsonl_file(validation_file)
    
    if not train_valid or not validation_valid:
        print("âŒ æ•°æ®æ–‡ä»¶éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼")
        return
    
    print(f"âœ… æ•°æ®éªŒè¯é€šè¿‡")
    print(f"   è®­ç»ƒæ•°æ®: {train_count} æ¡")
    print(f"   éªŒè¯æ•°æ®: {validation_count} æ¡")
    
    # æ£€æŸ¥å­˜å‚¨æ¡¶
    print(f"\nğŸª£ æ£€æŸ¥å­˜å‚¨æ¡¶...")
    if not check_bucket_exists(bucket_name):
        print("âŒ å­˜å‚¨æ¡¶ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥é…ç½®")
        return
    
    # ä¸Šä¼ æ•°æ®æ–‡ä»¶åˆ°GCS
    print(f"\nğŸ“¤ ä¸Šä¼ æ•°æ®æ–‡ä»¶åˆ°GCS...")
    
    train_gcs_path = "hotel_tuning/optimized_train.jsonl"
    validation_gcs_path = "hotel_tuning/optimized_validation.jsonl"
    
    if not upload_to_gcs(train_file, bucket_name, train_gcs_path):
        print("âŒ è®­ç»ƒæ•°æ®ä¸Šä¼ å¤±è´¥")
        return
    
    if not upload_to_gcs(validation_file, bucket_name, validation_gcs_path):
        print("âŒ éªŒè¯æ•°æ®ä¸Šä¼ å¤±è´¥")
        return
    
    # æ„å»ºGCS URI
    train_uri = f"gs://{bucket_name}/{train_gcs_path}"
    validation_uri = f"gs://{bucket_name}/{validation_gcs_path}"
    
    print(f"âœ… æ•°æ®æ–‡ä»¶ä¸Šä¼ å®Œæˆ")
    print(f"   è®­ç»ƒæ•°æ®URI: {train_uri}")
    print(f"   éªŒè¯æ•°æ®URI: {validation_uri}")
    
    # å¯åŠ¨å¾®è°ƒä»»åŠ¡
    print(f"\nğŸš€ å¯åŠ¨å¾®è°ƒä»»åŠ¡...")
    sft_tuning_job = start_tuning_job(project_id, location, train_uri, validation_uri)
    
    if not sft_tuning_job:
        print("âŒ å¾®è°ƒä»»åŠ¡å¯åŠ¨å¤±è´¥")
        return
    
    # ç›‘æ§å¾®è°ƒä»»åŠ¡
    print(f"\nâ³ ç›‘æ§å¾®è°ƒä»»åŠ¡...")
    if monitor_tuning_job(sft_tuning_job):
        print("âœ… å¾®è°ƒä»»åŠ¡æˆåŠŸå®Œæˆ")
        
        # æµ‹è¯•å¾®è°ƒåçš„æ¨¡å‹
        print(f"\nğŸ§ª æµ‹è¯•å¾®è°ƒåçš„æ¨¡å‹...")
        test_tuned_model(sft_tuning_job)
        
        print(f"\nğŸ‰ å¾®è°ƒæµç¨‹å…¨éƒ¨å®Œæˆï¼")
        print(f"ğŸ¨ æ‚¨çš„é…’åº—æœåŠ¡åŠ©æ‰‹æ¨¡å‹å·²å‡†å¤‡å°±ç»ª")
        
    else:
        print("âŒ å¾®è°ƒä»»åŠ¡å¤±è´¥")

if __name__ == "__main__":
    main() 